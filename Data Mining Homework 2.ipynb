{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b245ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth  precision    recall        f1\n",
      "0          1   0.468333  0.633333  0.519540\n",
      "1          2   0.969444  0.966667  0.966411\n",
      "2          3   1.000000  1.000000  1.000000\n",
      "3          4   1.000000  1.000000  1.000000\n",
      "4          5   1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "#problem 1\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the data\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='target')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train decision trees with different depths\n",
    "results = {'max_depth': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(\n",
    "        max_depth=depth,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Modified this line of code, added zero_division=0\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    weighted_avg = report['weighted avg']\n",
    "\n",
    "    results['max_depth'].append(depth)\n",
    "    results['precision'].append(weighted_avg['precision'])\n",
    "    results['recall'].append(weighted_avg['recall'])\n",
    "    results['f1'].append(weighted_avg['f1-score'])\n",
    "\n",
    "# Output the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ff07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First split feature: uniformity_cell_size\n",
      "First split threshold: 3.5\n",
      "Entropy before split: 0.9217431888789798\n",
      "Gini before split: 0.44674462982090646\n",
      "Misclassification error before split: 0.33682008368200833\n",
      "Entropy left after split: 0.3313056130577884\n",
      "Gini left after split: 0.11451516954193952\n",
      "Misclassification error left after split: 0.060975609756097615\n",
      "Entropy right after split: 0.32744491915447627\n",
      "Gini right after split: 0.11280000000000001\n",
      "Misclassification error right after split: 0.06000000000000005\n",
      "Information Gain (Entropy): 0.5916490906444065\n",
      "Information Gain (Gini): 0.3327676933988224\n",
      "Information Gain (Misclassification Error): 0.27615062761506265\n"
     ]
    }
   ],
   "source": [
    "#problem 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
    "column_names = ['id', 'clump_thickness', 'uniformity_cell_size', 'uniformity_cell_shape',\n",
    "                'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei',\n",
    "                'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class']\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# 2. Data preprocessing\n",
    "# Replace '?' in 'bare_nuclei' column with NaN, then drop rows containing NaN\n",
    "df['bare_nuclei'] = df['bare_nuclei'].replace('?', np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert 'bare_nuclei' column to integer type\n",
    "df['bare_nuclei'] = df['bare_nuclei'].astype(int)\n",
    "\n",
    "# Convert 'class' column to binary classification (2: benign, 4: malignant)\n",
    "df['class'] = df['class'].map({2: 0, 4: 1})\n",
    "\n",
    "# Drop 'id' column as it's irrelevant to classification\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "# 3. Split the dataset into training and testing sets\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 4. Train the decision tree\n",
    "dtc = DecisionTreeClassifier(max_depth=2, min_samples_leaf=2, min_samples_split=5, criterion='gini', random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 5. Get information about the first split\n",
    "first_split_feature_index = dtc.tree_.feature[0]\n",
    "first_split_feature_name = X.columns[first_split_feature_index]\n",
    "first_split_threshold = dtc.tree_.threshold[0]\n",
    "\n",
    "print(f\"First split feature: {first_split_feature_name}\")\n",
    "print(f\"First split threshold: {first_split_threshold}\")\n",
    "\n",
    "# 6. Calculate Entropy, Gini, Misclassification Error, and Information Gain for the first split\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"Calculate entropy\"\"\"\n",
    "    counts = Counter(y)\n",
    "    probabilities = [count / len(y) for count in counts.values()]\n",
    "    entropy = -sum([p * np.log2(p) for p in probabilities])\n",
    "    return entropy\n",
    "\n",
    "def calculate_gini(y):\n",
    "    \"\"\"Calculate Gini coefficient\"\"\"\n",
    "    counts = Counter(y)\n",
    "    probabilities = [count / len(y) for count in counts.values()]\n",
    "    gini = 1 - sum([p**2 for p in probabilities])\n",
    "    return gini\n",
    "\n",
    "def calculate_misclassification_error(y):\n",
    "    \"\"\"Calculate misclassification error\"\"\"\n",
    "    counts = Counter(y)\n",
    "    majority_class_count = max(counts.values())\n",
    "    misclassification_error = 1 - (majority_class_count / len(y))\n",
    "    return misclassification_error\n",
    "\n",
    "# Dataset before the first split\n",
    "entropy_before = calculate_entropy(y_train)\n",
    "gini_before = calculate_gini(y_train)\n",
    "misclassification_error_before = calculate_misclassification_error(y_train)\n",
    "\n",
    "print(f\"Entropy before split: {entropy_before}\")\n",
    "print(f\"Gini before split: {gini_before}\")\n",
    "print(f\"Misclassification error before split: {misclassification_error_before}\")\n",
    "\n",
    "# Split the dataset\n",
    "left_indices = X_train[first_split_feature_name] <= first_split_threshold\n",
    "right_indices = X_train[first_split_feature_name] > first_split_threshold\n",
    "\n",
    "X_left, y_left = X_train[left_indices], y_train[left_indices]\n",
    "X_right, y_right = X_train[right_indices], y_train[right_indices]\n",
    "\n",
    "# Dataset after the first split\n",
    "entropy_left = calculate_entropy(y_left)\n",
    "gini_left = calculate_gini(y_left)\n",
    "misclassification_error_left = calculate_misclassification_error(y_left)\n",
    "\n",
    "entropy_right = calculate_entropy(y_right)\n",
    "gini_right = calculate_gini(y_right)\n",
    "misclassification_error_right = calculate_misclassification_error(y_right)\n",
    "\n",
    "print(f\"Entropy left after split: {entropy_left}\")\n",
    "print(f\"Gini left after split: {gini_left}\")\n",
    "print(f\"Misclassification error left after split: {misclassification_error_left}\")\n",
    "\n",
    "print(f\"Entropy right after split: {entropy_right}\")\n",
    "print(f\"Gini right after split: {gini_right}\")\n",
    "print(f\"Misclassification error right after split: {misclassification_error_right}\")\n",
    "\n",
    "# Calculate Information Gain\n",
    "information_gain_entropy = entropy_before - (len(y_left) / len(y_train) * entropy_left + len(y_right) / len(y_train) * entropy_right)\n",
    "information_gain_gini = gini_before - (len(y_left) / len(y_train) * gini_left + len(y_right) / len(y_train) * gini_right)\n",
    "information_gain_misclassification_error = misclassification_error_before - (len(y_left) / len(y_train) * misclassification_error_left + len(y_right) / len(y_train) * misclassification_error_right)\n",
    "\n",
    "print(f\"Information Gain (Entropy): {information_gain_entropy}\")\n",
    "print(f\"Information Gain (Gini): {information_gain_gini}\")\n",
    "print(f\"Information Gain (Misclassification Error): {information_gain_misclassification_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192c4b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "Precision: 0.9047619047619048\n",
      "Recall: 0.9047619047619048\n",
      "F1 Score: 0.9047619047619048\n",
      "\n",
      "PCA with 1 component:\n",
      "Precision: 0.8787878787878788\n",
      "Recall: 0.9206349206349206\n",
      "F1 Score: 0.8992248062015504\n",
      "Confusion Matrix:\n",
      "[[100   8]\n",
      " [  5  58]]\n",
      "FP: 8\n",
      "TP: 58\n",
      "FPR: 0.07407407407407407\n",
      "TPR: 0.9206349206349206\n",
      "\n",
      "PCA with 2 components:\n",
      "Precision: 0.9152542372881356\n",
      "Recall: 0.8571428571428571\n",
      "F1 Score: 0.8852459016393442\n",
      "Confusion Matrix:\n",
      "[[103   5]\n",
      " [  9  54]]\n",
      "FP: 5\n",
      "TP: 54\n",
      "FPR: 0.046296296296296294\n",
      "TPR: 0.8571428571428571\n",
      "\n",
      "Comparison:\n",
      "Original F1: 0.9047619047619048, PCA 1 F1: 0.8992248062015504, PCA 2 F1: 0.8852459016393442\n",
      "Using continuous data with PCA is NOT beneficial in this case.\n"
     ]
    }
   ],
   "source": [
    "#problem 3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# 1. Load the dataset (using ucimlrepo)\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "# Convert y to a numpy array and flatten it\n",
    "y = np.ravel(y)\n",
    "\n",
    "# Convert string labels to numerical labels\n",
    "y = np.where(y == 'M', 1, 0)\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Feature scaling (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Train the original data model\n",
    "dtc_original = DecisionTreeClassifier(max_depth=2, min_samples_leaf=2, min_samples_split=5, random_state=42)\n",
    "dtc_original.fit(X_train_scaled, y_train)  # Use scaled data\n",
    "y_pred_original = dtc_original.predict(X_test_scaled) # Use scaled data\n",
    "\n",
    "precision_original = precision_score(y_test, y_pred_original)\n",
    "recall_original = recall_score(y_test, y_pred_original)\n",
    "f1_original = f1_score(y_test, y_pred_original)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(f\"Precision: {precision_original}\")\n",
    "print(f\"Recall: {recall_original}\")\n",
    "print(f\"F1 Score: {f1_original}\")\n",
    "\n",
    "# 5. PCA dimensionality reduction\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# 6. Use 1 principal component\n",
    "X_train_pca_1 = X_train_pca[:, :1]\n",
    "X_test_pca_1 = X_test_pca[:, :1]\n",
    "\n",
    "dtc_pca_1 = DecisionTreeClassifier(max_depth=2, min_samples_leaf=2, min_samples_split=5, random_state=42)\n",
    "dtc_pca_1.fit(X_train_pca_1, y_train)\n",
    "y_pred_pca_1 = dtc_pca_1.predict(X_test_pca_1)\n",
    "\n",
    "precision_pca_1 = precision_score(y_test, y_pred_pca_1)\n",
    "recall_pca_1 = recall_score(y_test, y_pred_pca_1)\n",
    "f1_pca_1 = f1_score(y_test, y_pred_pca_1)\n",
    "\n",
    "cm_pca_1 = confusion_matrix(y_test, y_pred_pca_1)\n",
    "FP_pca_1 = cm_pca_1[0, 1]\n",
    "TP_pca_1 = cm_pca_1[1, 1]\n",
    "FPR_pca_1 = FP_pca_1 / (cm_pca_1[0, 1] + cm_pca_1[0, 0]) if (cm_pca_1[0, 1] + cm_pca_1[0, 0]) != 0 else 0\n",
    "TPR_pca_1 = TP_pca_1 / (cm_pca_1[1, 1] + cm_pca_1[1, 0]) if (cm_pca_1[1, 1] + cm_pca_1[1, 0]) != 0 else 0\n",
    "\n",
    "print(\"\\nPCA with 1 component:\")\n",
    "print(f\"Precision: {precision_pca_1}\")\n",
    "print(f\"Recall: {recall_pca_1}\")\n",
    "print(f\"F1 Score: {f1_pca_1}\")\n",
    "print(f\"Confusion Matrix:\\n{cm_pca_1}\")\n",
    "print(f\"FP: {FP_pca_1}\")\n",
    "print(f\"TP: {TP_pca_1}\")\n",
    "print(f\"FPR: {FPR_pca_1}\")\n",
    "print(f\"TPR: {TPR_pca_1}\")\n",
    "\n",
    "# 7. Use 2 principal components\n",
    "X_train_pca_2 = X_train_pca[:, :2]\n",
    "X_test_pca_2 = X_test_pca[:, :2]\n",
    "\n",
    "dtc_pca_2 = DecisionTreeClassifier(max_depth=2, min_samples_leaf=2, min_samples_split=5, random_state=42)\n",
    "dtc_pca_2.fit(X_train_pca_2, y_train)\n",
    "y_pred_pca_2 = dtc_pca_2.predict(X_test_pca_2)\n",
    "\n",
    "precision_pca_2 = precision_score(y_test, y_pred_pca_2)\n",
    "recall_pca_2 = recall_score(y_test, y_pred_pca_2)\n",
    "f1_pca_2 = f1_score(y_test, y_pred_pca_2)\n",
    "\n",
    "cm_pca_2 = confusion_matrix(y_test, y_pred_pca_2)\n",
    "FP_pca_2 = cm_pca_2[0, 1]\n",
    "TP_pca_2 = cm_pca_2[1, 1]\n",
    "FPR_pca_2 = FP_pca_2 / (cm_pca_2[0, 1] + cm_pca_2[0, 0]) if (cm_pca_2[0, 1] + cm_pca_2[0, 0]) != 0 else 0\n",
    "TPR_pca_2 = TP_pca_2 / (cm_pca_2[1, 1] + cm_pca_2[1, 0]) if (cm_pca_2[1, 1] + cm_pca_2[1, 0]) != 0 else 0\n",
    "\n",
    "print(\"\\nPCA with 2 components:\")\n",
    "print(f\"Precision: {precision_pca_2}\")\n",
    "print(f\"Recall: {recall_pca_2}\")\n",
    "print(f\"F1 Score: {f1_pca_2}\")\n",
    "print(f\"Confusion Matrix:\\n{cm_pca_2}\")\n",
    "print(f\"FP: {FP_pca_2}\")\n",
    "print(f\"TP: {TP_pca_2}\")\n",
    "print(f\"FPR: {FPR_pca_2}\")\n",
    "print(f\"TPR: {TPR_pca_2}\")\n",
    "\n",
    "# 8. Compare the results\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Original F1: {f1_original}, PCA 1 F1: {f1_pca_1}, PCA 2 F1: {f1_pca_2}\")\n",
    "\n",
    "# 9. Determine if using continuous data is beneficial\n",
    "if f1_pca_1 > f1_original or f1_pca_2 > f1_original:\n",
    "    print(\"Using continuous data with PCA is beneficial in this case.\")\n",
    "else:\n",
    "    print(\"Using continuous data with PCA is NOT beneficial in this case.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-nnU-Net] *",
   "language": "python",
   "name": "conda-env-anaconda3-nnU-Net-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
